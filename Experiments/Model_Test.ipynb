{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install segmentation-models-pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7rpdObn4Wc2",
        "outputId": "f60d1dc5-ecd0-417e-fa22-5f3dcca5c86b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: segmentation-models-pytorch in /usr/local/lib/python3.10/dist-packages (0.3.3)\n",
            "Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (0.16.0+cu118)\n",
            "Requirement already satisfied: pretrainedmodels==0.7.4 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (0.7.4)\n",
            "Requirement already satisfied: efficientnet-pytorch==0.7.1 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (0.7.1)\n",
            "Requirement already satisfied: timm==0.9.2 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (0.9.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (4.66.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (9.4.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.1.0+cu118)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.10/dist-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (4.0.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation-models-pytorch) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation-models-pytorch) (0.18.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation-models-pytorch) (0.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (2.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.1.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzDnFE1Rx8lY"
      },
      "outputs": [],
      "source": [
        "# Importing Libraries\n",
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining Constants\n",
        "BATCH_SIZE = 1\n",
        "RADIUS = 5\n",
        "THRESHOLD = 0.5\n",
        "CLASSES = [\"Fields\"]\n",
        "IMG_EXTNS = [\"png\", \"jpg\", \"jpeg\"]\n",
        "DATA_DIR = \"/content/drive/Shareddrives/Street2Sat4/S2S_Depth/valid/\"\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "JSON_PATH = \"/content/drive/Shareddrives/Street2Sat4/S2S_Depth_COCO/valid/_annotations.coco.json\"\n",
        "MODEL_NAME = \"DepthExperiments512x512xception\"\n",
        "MODEL_DIR = \"/content/drive/Shareddrives/Street2Sat4/Distance_Models/\" + MODEL_NAME + \"/\"\n",
        "OUT_DIR = \"/content/drive/Shareddrives/Street2Sat4/Depth_Centr_Results/\" + MODEL_NAME + \"/\""
      ],
      "metadata": {
        "id": "ujYxRkr4yH-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper Functions\n",
        "\n",
        "def to_tensor(x, **kwargs):\n",
        "    return torch.from_numpy(x.transpose(2, 0, 1).astype(\"float32\"))\n",
        "\n",
        "\n",
        "def preprocess(img, **kwargs):\n",
        "    img = img.astype(float)\n",
        "    img = (\n",
        "        255 * (img - np.min(img[:])) / (np.max(img[:]) - np.min(img[:]) + 0.1)\n",
        "    ).astype(float)\n",
        "    img = (img + 0.5) / 256\n",
        "    gamma = -1 / np.nanmean(np.log(img))\n",
        "    img = img ** (gamma)\n",
        "    return img\n",
        "\n",
        "\n",
        "# Post Processing\n",
        "def getLargestContour(pred):\n",
        "  cnts, _ = cv2.findContours((pred>0.5).astype(np.uint8), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "  if(len(cnts)==0):\n",
        "    return pred\n",
        "  cnt  = max(cnts, key = lambda x: cv2.contourArea(x))\n",
        "  new_img = np.zeros(pred.shape)\n",
        "  cv2.fillPoly(new_img, pts = [cnt], color=(1.0,1.0,1.0))\n",
        "  return new_img * pred\n",
        "\n",
        "\n",
        "# Representative Point Extraction\n",
        "def getNewMarkPos(img):\n",
        "  x, y = np.where(img>THRESHOLD)\n",
        "  if(len(x)==0):\n",
        "    print(f\"{len(x)=}\")\n",
        "    return np.unravel_index(np.argmax(img),img.shape)[::-1]\n",
        "  x_centr, y_centr = np.mean(x), np.mean(y)\n",
        "  med_val = np.median(img[x,y])\n",
        "  x_new, y_new = np.where(img>=med_val)\n",
        "  z = [i for i in zip(x_new,y_new)]\n",
        "  return min(z, key = lambda k : (x_centr - k[0])**2 + (y_centr - k[1])**2)\n",
        "\n",
        "\n",
        "def markImg(img):\n",
        "  z = np.unravel_index(np.argmax(img),img.shape)[::-1]\n",
        "  new_img = np.ones((*img.shape,3)) * img.reshape((*img.shape,1))\n",
        "  cv2.circle(new_img, z, RADIUS, (0.0,1.0,0.0), -1)\n",
        "  y1,x1 = getNewMarkPos(img)\n",
        "  cv2.circle(new_img, (x1,y1), RADIUS, (0.0,0.0,1.0), -1)\n",
        "  return new_img\n",
        "\n",
        "\n",
        "def getZeroDist(img,pt):\n",
        "  x,y = pt\n",
        "  y1,x1 = np.where(img==0)\n",
        "  dists = np.sqrt(((x-x1)**2 + (y-y1)**2))\n",
        "  return dists.min()\n",
        "\n",
        "def calcDists(img):\n",
        "  img = np.pad(img,1)\n",
        "  y0,x0 = np.unravel_index(np.argmax(img),img.shape)\n",
        "  dist0 = getZeroDist(img,(x0,y0))\n",
        "  y1,x1 = getNewMarkPos(img)\n",
        "  dist1 = getZeroDist(img,(x1,y1))\n",
        "  return [dist0,dist1]\n",
        "\n",
        "\n",
        "class Data(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        images_dir,\n",
        "        classes=None,\n",
        "        augmentation=None,\n",
        "        preprocessing=None,\n",
        "        json_location=None,\n",
        "        device = \"cpu\"\n",
        "    ):\n",
        "        self.ids = os.listdir(images_dir)\n",
        "        self.images_fps = sorted(\n",
        "            [\n",
        "                os.path.join(images_dir, image_id)\n",
        "                for image_id in self.ids\n",
        "                if image_id.split(\".\")[-2].split(\"_\")[-1] != \"mask\"\n",
        "                and image_id.split(\".\") != \"csv\"\n",
        "            ]\n",
        "        )\n",
        "        self.masks_fps = sorted(\n",
        "            [\n",
        "                os.path.join(images_dir, image_id)\n",
        "                for image_id in self.ids\n",
        "                if image_id.split(\".\")[-2].split(\"_\")[-1] == \"mask\"\n",
        "            ]\n",
        "        )\n",
        "        self.img_size = (1024, 512)\n",
        "        self.preprocessing = preprocessing\n",
        "        self.json_location = json_location\n",
        "        self.device = DEVICE\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        image = cv2.imread(self.images_fps[i])\n",
        "        mask = cv2.imread(self.masks_fps[i], cv2.IMREAD_GRAYSCALE).astype(\"float\")\n",
        "        mask = mask[0:512, 0:512]\n",
        "        image = image[0:512, 0:512]\n",
        "        if self.json_location:\n",
        "            with open(self.json_location) as f:\n",
        "                img_json = json.load(f)\n",
        "                for img in img_json[\"images\"]:\n",
        "                    if img[\"id\"] == i:\n",
        "                        for j in img[\"extra\"][\"user_tags\"]:\n",
        "                            if j == \"Right\":\n",
        "                                continue\n",
        "\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        mask = mask.reshape((*mask.shape,1))\n",
        "        \"\"\"\n",
        "        masks = [(mask == v) for v in self.class_values]\n",
        "        mask = np.stack(masks, axis=-1).astype(\"float\")\n",
        "        \"\"\"\n",
        "\n",
        "        if self.preprocessing:\n",
        "            sample = self.preprocessing(image=image, mask=mask)\n",
        "            image, mask = sample[\"image\"], sample[\"mask\"]\n",
        "        image = preprocess(image)\n",
        "        mask = preprocess(mask)\n",
        "        return to_tensor(image).to(self.device), to_tensor(mask).to(self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.masks_fps)"
      ],
      "metadata": {
        "id": "XAZGMzijyYic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not(os.path.isdir(OUT_DIR)):\n",
        "  os.makedirs(OUT_DIR)"
      ],
      "metadata": {
        "id": "rOBLB_M3PYiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = Data(\n",
        "    images_dir = DATA_DIR,\n",
        "    classes = CLASSES,\n",
        "    json_location = JSON_PATH,\n",
        "    device = DEVICE,\n",
        ")\n",
        "\n",
        "val_data = DataLoader(val_dataset, batch_size = BATCH_SIZE, shuffle = False)\n",
        "\n",
        "model = torch.load(MODEL_DIR + \"/best_model.pth\").to(DEVICE)"
      ],
      "metadata": {
        "id": "ouspyq9Y0uLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dists0 = []\n",
        "dists1 = []\n",
        "for num,(img, mask) in enumerate(val_data):\n",
        "  print(f\"\\rWorking on img {num+1}/{len(val_data)}\", end=\"\")\n",
        "  pred = model(img)\n",
        "  mask_new = mask.cpu().numpy()[0,0]\n",
        "  cv2.imwrite(OUT_DIR + f\"{str(num).zfill(5)}_mask.png\",mask_new*255)\n",
        "  img_new = img.cpu()[0].numpy().transpose(1, 2, 0)\n",
        "  cv2.imwrite(OUT_DIR + f\"{str(num).zfill(5)}_img.png\",img_new*255)\n",
        "  pred_new = pred.detach().cpu().numpy()[0,0]\n",
        "  pred_new = getLargestContour(pred_new)\n",
        "  # Dist estimation is only for internal analytics\n",
        "  dist0,dist1 = calcDists(pred_new)\n",
        "  pred_new = markImg(pred_new)\n",
        "  dists0.append(dist0)\n",
        "  dists1.append(dist1)\n",
        "  cv2.imwrite(OUT_DIR + f\"{str(num).zfill(5)}_pred.png\",pred_new*255)"
      ],
      "metadata": {
        "id": "convq-HH2aDn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "008c8eda-6687-40c3-8f11-c32d43ef6e78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working on img 55/352len(x)=0\n",
            "len(x)=0\n",
            "Working on img 57/352len(x)=0\n",
            "len(x)=0\n",
            "Working on img 136/352len(x)=0\n",
            "len(x)=0\n",
            "Working on img 138/352len(x)=0\n",
            "len(x)=0\n",
            "Working on img 139/352len(x)=0\n",
            "len(x)=0\n",
            "Working on img 140/352len(x)=0\n",
            "len(x)=0\n",
            "Working on img 158/352len(x)=0\n",
            "len(x)=0\n",
            "Working on img 194/352len(x)=0\n",
            "len(x)=0\n",
            "Working on img 330/352len(x)=0\n",
            "len(x)=0\n",
            "Working on img 352/352"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dists0 = np.array(dists0)\n",
        "print(f\"Method 0 (Max Pick): Mean : {np.mean(dists0)}, Median: {np.median(dists0)}\")\n",
        "dists1 = np.array(dists1)\n",
        "print(f\"Method 1 (Median Filtered Closest to Centroid Pick): Mean : {np.mean(dists1)}, Median: {np.median(dists1)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pcMxQNoEUsY",
        "outputId": "886ae9d7-d6b4-4152-8100-9179a747cf33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method 0 (Max Pick): Mean : 4.224850513544166, Median: 3.0\n",
            "Method 1 (Median Filtered Closest to Centroid Pick): Mean : 61.0033298711035, Median: 51.351572302198896\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EfficientNet-b5<br>\n",
        "Working on img 23/352len(x)=0<br>\n",
        "Working on img 52/352len(x)=0<br>\n",
        "Working on img 81/352len(x)=0<br>\n",
        "Working on img 284/352len(x)=0<br>\n",
        "Working on img 329/352len(x)=0<br>\n",
        "Working on img 337/352len(x)=0<br>\n",
        "Working on img 351/352<br>\n",
        "Method 0 (Max Pick): Mean : 9.054178629356322, Median: 4.0<br>\n",
        "Method 1 (Median Filtered Closest to Centroid Pick): Mean : 65.07702971240862, Median: 58.914310589820275<br>"
      ],
      "metadata": {
        "id": "j_nArwDXUr2k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DenseNet161<br>\n",
        "Working on img 11/352len(x)=0<br>\n",
        "Working on img 56/352len(x)=0<br>\n",
        "Working on img 165/352len(x)=0<br>\n",
        "Working on img 337/352len(x)=0<br>\n",
        "Working on img 351/352<br>\n",
        "Method 0 (Max Pick): Mean : 3.398718174943056, Median: 3.0<br>\n",
        "Method 1 (Median Filtered Closest to Centroid Pick): Mean : 65.74666391827189, Median: 57.53506614676055<br>\n"
      ],
      "metadata": {
        "id": "7x8RV0mNTxbA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Xception<br>\n",
        "Working on img 55/352len(x)=0<br>\n",
        "Working on img 57/352len(x)=0<br>\n",
        "Working on img 136/352len(x)=0<br>\n",
        "Working on img 138/352len(x)=0<br>\n",
        "Working on img 139/352len(x)=0<br>\n",
        "Working on img 140/352len(x)=0<br>\n",
        "Working on img 158/352len(x)=0<br>\n",
        "Working on img 194/352len(x)=0<br>\n",
        "Working on img 330/352len(x)=0<br>\n",
        "Method 0 (Max Pick): Mean : 4.224850513544166, Median: 3.0<br>\n",
        "Method 1 (Median Filtered Closest to Centroid Pick): Mean : 61.0033298711035, Median: 51.351572302198896<br>"
      ],
      "metadata": {
        "id": "7T_aO9LSiJ3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/drive/Shareddrives/Street2Sat4/Depth_Centr_Results /content/drive/Shareddrives/Street2Sat_Updates/"
      ],
      "metadata": {
        "id": "TAuN9K3sVTI1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}