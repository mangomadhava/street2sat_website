{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c00d52b5",
   "metadata": {},
   "source": [
    "# Disabling Autoshape \n",
    "\n",
    "The yolov5 model takes an image as input. The AutoShape functionality allows this image to be of any size and type, however because of this it's not possible to convert the end-to-end model with AutoShape into TorchScript.\n",
    "\n",
    "This notebook takes out the appropriate parts of AutoShape into functions and verifies the behaviour is unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf0e3063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'../street2sat_utils/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4828e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from yolov5 import hubconf\n",
    "from yolov5.utils.datasets import letterbox\n",
    "from yolov5.utils.general import make_divisible, non_max_suppression, scale_coords\n",
    "from yolov5.models.common import Detections, AutoShape\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bda9c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2028, 2704, 3) (2028, 2704, 3) (2028, 2704, 3)\n"
     ]
    }
   ],
   "source": [
    "img1_path = '../example_images/GP__1312.JPG'\n",
    "img2_path = '../example_images/GP__1313.JPG'\n",
    "img3_path = '../example_images/GP__1314.JPG'\n",
    "\n",
    "img1 = cv2.cvtColor(cv2.imread(img1_path), cv2.COLOR_BGR2RGB)\n",
    "img2 = cv2.cvtColor(cv2.imread(img2_path), cv2.COLOR_BGR2RGB)\n",
    "img3 = cv2.cvtColor(cv2.imread(img3_path), cv2.COLOR_BGR2RGB)\n",
    "print(img1.shape, img2.shape, img3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23895882",
   "metadata": {},
   "source": [
    "## Loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "150be00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "/Users/izvonkov/nasaharvest/street2sat_website/venv/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ../c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "Model Summary: 391 layers, 21100857 parameters, 0 gradients\n",
      "Adding AutoShape... \n",
      "YOLOv5 ðŸš€ d4c76ee torch 1.9.0 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 391 layers, 21100857 parameters, 0 gradients\n",
      "YOLOv5 ðŸš€ d4c76ee torch 1.9.0 CPU\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_auto = hubconf.custom(\"../model_weights/yolo/best.pt\", autoshape=True)\n",
    "model_no = hubconf.custom(\"../model_weights/yolo/best.pt\", autoshape=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490a2dc5",
   "metadata": {},
   "source": [
    "## Original Model with AutoShape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "979524cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>confidence</th>\n",
       "      <th>class</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>474.721436</td>\n",
       "      <td>834.870789</td>\n",
       "      <td>646.257874</td>\n",
       "      <td>1155.997437</td>\n",
       "      <td>0.654839</td>\n",
       "      <td>11</td>\n",
       "      <td>sugarcane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1242.162109</td>\n",
       "      <td>750.757385</td>\n",
       "      <td>1398.083252</td>\n",
       "      <td>1085.277710</td>\n",
       "      <td>0.628839</td>\n",
       "      <td>11</td>\n",
       "      <td>sugarcane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1986.495850</td>\n",
       "      <td>783.639343</td>\n",
       "      <td>2100.967285</td>\n",
       "      <td>1043.109863</td>\n",
       "      <td>0.604560</td>\n",
       "      <td>11</td>\n",
       "      <td>sugarcane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1842.620483</td>\n",
       "      <td>786.651917</td>\n",
       "      <td>1964.361328</td>\n",
       "      <td>1052.111084</td>\n",
       "      <td>0.593445</td>\n",
       "      <td>11</td>\n",
       "      <td>sugarcane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>787.024536</td>\n",
       "      <td>837.682739</td>\n",
       "      <td>903.177734</td>\n",
       "      <td>1124.324341</td>\n",
       "      <td>0.581062</td>\n",
       "      <td>11</td>\n",
       "      <td>sugarcane</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          xmin        ymin         xmax         ymax  confidence  class  \\\n",
       "0   474.721436  834.870789   646.257874  1155.997437    0.654839     11   \n",
       "1  1242.162109  750.757385  1398.083252  1085.277710    0.628839     11   \n",
       "2  1986.495850  783.639343  2100.967285  1043.109863    0.604560     11   \n",
       "3  1842.620483  786.651917  1964.361328  1052.111084    0.593445     11   \n",
       "4   787.024536  837.682739   903.177734  1124.324341    0.581062     11   \n",
       "\n",
       "        name  \n",
       "0  sugarcane  \n",
       "1  sugarcane  \n",
       "2  sugarcane  \n",
       "3  sugarcane  \n",
       "4  sugarcane  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_1 = model_auto(img1).pandas().xyxy[0]\n",
    "result_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564492b4",
   "metadata": {},
   "source": [
    "## AutoShape used explicitly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ccc9770",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = AutoShape(model_no)\n",
    "m.stride = model_no.stride\n",
    "m.names = model_no.names\n",
    "result_2 = m(img1).pandas().xyxy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa68494d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_1.equals(result_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405d7322",
   "metadata": {},
   "source": [
    "## Reproducing Autoshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a65d6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproduce preprocessing from AutoShape https://github.com/ultralytics/yolov5/blob/master/models/common.py\n",
    "size = 640\n",
    "p = next(model_no.parameters()) # Can get from jit\n",
    "max_stride = int(model_no.stride.max()) # Must hardcode\n",
    "\n",
    "def preprocess(img_path):\n",
    "    img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "    s = img.shape[:2]\n",
    "    g = (size / max(s))\n",
    "    shape1 = [[y * g for y in s]]\n",
    "    shape1 = [make_divisible(x, max_stride) for x in np.stack(shape1, 0).max(0)]  # inference shape\n",
    "    x = letterbox(img1, new_shape=shape1, auto=False)[0]   # pad\n",
    "    x = np.ascontiguousarray(x[None].transpose((0, 3, 1, 2)))  # BHWC to BCHW\n",
    "    img_tensor = torch.from_numpy(x).to(p.device).type_as(p) / 255.  # uint8 to fp16/32\n",
    "    return img, img_tensor, shape1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fac0aa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = 0.25\n",
    "iou = 0.45\n",
    "classes = None\n",
    "max_det = 1000\n",
    "\n",
    "def postprocess(img, shape1, output, img_tensor):\n",
    "    \n",
    "    # Inference\n",
    "    y = output[0]  # forward\n",
    "\n",
    "    # Post-process\n",
    "    y = non_max_suppression(y, conf, iou_thres=iou, classes=classes, max_det=max_det)  # NMS\n",
    "\n",
    "    scale_coords(shape1, y[0][:, :4], img.shape[:2])\n",
    "    \n",
    "    return Detections(imgs=[img], pred=y, files=[], times=[1,2,3,4], names=model_no.names, shape=img_tensor.shape\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "87f78c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_original, img_tensor, shape1 = preprocess(img1_path)\n",
    "output = model_no(img_tensor)\n",
    "detections = postprocess(img_original, shape1, output, img_tensor)\n",
    "result_3 = detections.pandas().xyxy[0]\n",
    "result_1.equals(result_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995d45c6",
   "metadata": {},
   "source": [
    "## Saving as TorchScript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5c046884",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/izvonkov/nasaharvest/street2sat_website/venv/lib/python3.7/site-packages/yolov5/models/yolo.py:55: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if self.grid[i].shape[2:4] != x[i].shape[2:4] or self.onnx_dynamic:\n"
     ]
    }
   ],
   "source": [
    "model_path = \"../street2sat_utils/model_weights/best.torchscript.pt\"\n",
    "ts = torch.jit.trace(model_no, img_tensor, strict=False)\n",
    "ts.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8d13dad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_jit = torch.jit.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e543e634",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_jit = model_jit(img_tensor)\n",
    "detections = postprocess(img1, shape1, output_jit, img_tensor)\n",
    "result_4 = detections.pandas().xyxy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b32f35e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_1.equals(result_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa47f82c",
   "metadata": {},
   "source": [
    "## Comparing to:\n",
    "https://github.com/louisoutin/yolov5_torchserve/blob/master/ressources/torchserve_handler.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2fb272ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(32.)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_no.stride.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d03738",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
